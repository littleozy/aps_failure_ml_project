{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_process.ipynb\n",
      "importing Jupyter notebook from functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from data_process import X_train, y_train, X_test, y_test\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features separating\n",
    "\n",
    "From the data description, the features can be classified into two categories: histogram and numerical.\n",
    "\n",
    "For the features, the prefix is the Identifier and the suffix is the bin_id. aOnly the histogram features will have bin value greater than 0 (if all features follow the same naming convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_col = []\n",
    "for i in X_train.columns:\n",
    "  if i[-1] != '0':\n",
    "    hist_col.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_features = X_train[hist_col]\n",
    "num_features = X_train.drop(hist_col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram features analysis\n",
    "\n",
    "Using Recursive feature elimination to select the top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature_hist = get_top(hist_features,y_train,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature_hist_hardcode = ['ag_001',\n",
    " 'ag_002',\n",
    " 'ag_003',\n",
    " 'ag_006',\n",
    " 'ay_005',\n",
    " 'ay_006',\n",
    " 'ba_003',\n",
    " 'ba_004',\n",
    " 'ba_005',\n",
    " 'cn_001',\n",
    " 'cn_004',\n",
    " 'cs_002',\n",
    " 'cs_004',\n",
    " 'ee_002',\n",
    " 'ee_005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = pd.DataFrame(data=X_train[top_feature_hist],columns=top_feature_hist)\n",
    "top_features['class'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature_num = get_top(num_features,y_train,15) # this will take very long to run, refer to the hardcode below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature_num_hardcode = ['aa_000',\n",
    " 'ai_000',\n",
    " 'al_000',\n",
    " 'am_0',\n",
    " 'aq_000',\n",
    " 'bb_000',\n",
    " 'bj_000',\n",
    " 'bt_000',\n",
    " 'bv_000',\n",
    " 'ci_000',\n",
    " 'ck_000',\n",
    " 'cl_000',\n",
    " 'cn_000',\n",
    " 'cq_000',\n",
    " 'dn_000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = pd.DataFrame(data=X_train[top_feature_num],columns=top_feature_num)\n",
    "top_features['class'] = y_train\n",
    "top_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different imputation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: mean , Model: LGBM 23490\n",
      "Strategy: mean , Model: Gradient Boosted Decision Tree 19730\n",
      "Strategy: median , Model: LGBM 25120\n",
      "Strategy: median , Model: Gradient Boosted Decision Tree 18990\n",
      "Strategy: most_frequent , Model: LGBM 24430\n",
      "Strategy: most_frequent , Model: Gradient Boosted Decision Tree 20230\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "strat = ['mean','median','most_frequent']\n",
    "\n",
    "for i in strat:\n",
    "    X_train_imputed = impute(X_train,i)\n",
    "    X_test_imputed = impute(X_test,i)\n",
    "    X_train_smote, y_train_smote = balance_data(X_train_imputed,y_train)\n",
    "\n",
    "    iter = 1\n",
    "    cost_lgbm = 0\n",
    "    cost_gradient = 0\n",
    "\n",
    "    #while iter <= 5:\n",
    "        \n",
    "    cost_lgbm = cost_lgbm + LGBM(X_train_smote,y_train_smote,X_test_imputed,y_test)\n",
    "    cost_gradient = cost_gradient + gradient(X_train_smote,y_train_smote,X_test_imputed,y_test)\n",
    "\n",
    "        #iter += 1\n",
    "    \n",
    "    #cost_lgbm = cost_lgbm/(iter-1)\n",
    "    #cost_gradient = cost_gradient/(iter-1)\n",
    "\n",
    "    print('Strategy:',i,',', 'Model: LGBM',cost_lgbm)\n",
    "    print('Strategy:',i,',','Model: Gradient Boosted Decision Tree',cost_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know median is the best, will use median for the imputation strategy. Splitting the training data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = impute(X_train,'median')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_feature, X_test_feature, y_train_feature, y_test_feature = train_test_split(X_train_imputed,y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "Do not need scaling for random forests and decision trees as these models are scale-invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "strat = 'median'\n",
    "X_train_imputed = impute(X_train,strat)\n",
    "X_test_imputed = impute(X_test,strat)\n",
    "X_train_smote, y_train_smote = balance_data(X_train_imputed,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 170) (12000, 170)\n",
      "Model: LGBM 4717710\n",
      "Model: Gradient Boosted Decision Tree 4664520\n"
     ]
    }
   ],
   "source": [
    "X_train_scale,scaler = scale_data_minmax(X_train_feature)\n",
    "X_test_scale = pd.DataFrame(data = scaler.transform(X_test_feature) , columns=X_test_feature.columns)\n",
    "print(X_train_scale.shape,X_test_scale.shape)\n",
    "\n",
    "cost_lgbm = 0\n",
    "cost_gradient = 0\n",
    "iter = 1\n",
    "\n",
    "X_train_smote, y_train_smote = balance_data(X_train_scale,y_train_feature)\n",
    "#while iter <= 5:\n",
    "        \n",
    "cost_lgbm = cost_lgbm + LGBM(X_train_smote,y_train_smote,X_test_scale,y_test_feature)\n",
    "cost_gradient = cost_gradient + gradient(X_train_smote,y_train_smote,X_test_scale,y_test_feature)\n",
    "    #print(iter)\n",
    "    #iter += 1\n",
    "\n",
    "#cost_lgbm = cost_lgbm/(iter-1)\n",
    "#cost_gradient = cost_gradient/(iter-1)\n",
    "\n",
    "print('Model: LGBM',cost_lgbm)\n",
    "print('Model: Gradient Boosted Decision Tree',cost_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 170) (12000, 170)\n",
      "Model: LGBM 4719170\n",
      "Model: Gradient Boosted Decision Tree 4660650\n"
     ]
    }
   ],
   "source": [
    "X_train_scale,scaler = scale_data_standard(X_train_feature)\n",
    "X_test_scale = pd.DataFrame(data = scaler.transform(X_test_feature) , columns=X_test_feature.columns)\n",
    "print(X_train_scale.shape,X_test_scale.shape)\n",
    "\n",
    "cost_lgbm = 0\n",
    "cost_gradient = 0\n",
    "iter = 1\n",
    "\n",
    "X_train_smote, y_train_smote = balance_data(X_train_scale,y_train_feature)\n",
    "\n",
    "#while iter <= 5:\n",
    "        \n",
    "cost_lgbm = cost_lgbm + LGBM(X_train_smote,y_train_smote,X_test_scale,y_test_feature)\n",
    "cost_gradient = cost_gradient + gradient(X_train_smote,y_train_smote,X_test_scale,y_test_feature)\n",
    "    #print(iter)\n",
    "    #iter += 1\n",
    "\n",
    "#cost_lgbm = cost_lgbm/(iter-1)\n",
    "#cost_gradient = cost_gradient/(iter-1)\n",
    "\n",
    "print('Model: LGBM',cost_lgbm)\n",
    "print('Model: Gradient Boosted Decision Tree',cost_gradient)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d51f260eded27f650258a4de4708081dac957813be8f781887b52b18819514a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
